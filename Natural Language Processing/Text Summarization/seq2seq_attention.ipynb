{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PKnSKiDO4Toi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\" # Set the GPU 2 to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1R5q7J1w4Toi"
   },
   "source": [
    "### 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nEvc7y8PBe8u"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bXyrBVORBha3"
   },
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yNrlKYdX4Toj"
   },
   "outputs": [],
   "source": [
    "Train_data = pd.read_csv(\"open/train_data.csv\")\n",
    "Test_data = pd.read_csv(\"open/test_data.csv\")\n",
    "sample_submission = pd.read_csv(\"open/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gs_M3Chf4Tol",
    "outputId": "b763199b-1c18-45cb-caa4-fac90d321644"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271665, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "l5SGOr2s4Tok",
    "outputId": "2b7792cf-a8c9-4ad6-d787-6e9f7acce0d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>원고가 소속회사의 노동조합에서 분규가 발생하자 노조활동을 구실로 정상적인 근무를 해...</td>\n",
       "      <td>원고가  주동하여 회사업무능률을 저해하고 회사업무상의 지휘명령에 위반하였다면 이에 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>수출입업체인 원고가 의류제품을 제조ㆍ수출함에 있어 같은 그룹내 종합무역상사인 소외 ...</td>\n",
       "      <td>수출입업체인 원고가 의류제품을 제조ㆍ수출함에 있어 소외 회사의 직수출실적을 지원하기...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>가등기담보권자가 제소전 화해조항에 따라 자기 명의로 소유권이전의 본등기를 경료한 후...</td>\n",
       "      <td>가등기담보권자가 제소전 화해조항에 의해 자기 명의로 소유권이전의 본등기를 경료하고 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>가. 부가가치세법 제22조 제3항 단서에 제1호와 제2호가 동시에 해당한다는 뜻은 ...</td>\n",
       "      <td>부가가치세법 제22조 제3항 단서에 제1호와 제2호가 동시에 해당한다는 의미는 제1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>소득세법 제116조 제1항의 규정에 의하면 정부는 과세표준확정신고를 하여야 할 자에...</td>\n",
       "      <td>소득세법 제116조 제1항에 따르면 정부는 과세표준확정신고를 해야 할 자에 대해 당...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0   0  원고가 소속회사의 노동조합에서 분규가 발생하자 노조활동을 구실로 정상적인 근무를 해...   \n",
       "1   1  수출입업체인 원고가 의류제품을 제조ㆍ수출함에 있어 같은 그룹내 종합무역상사인 소외 ...   \n",
       "2   2  가등기담보권자가 제소전 화해조항에 따라 자기 명의로 소유권이전의 본등기를 경료한 후...   \n",
       "3   3  가. 부가가치세법 제22조 제3항 단서에 제1호와 제2호가 동시에 해당한다는 뜻은 ...   \n",
       "4   4  소득세법 제116조 제1항의 규정에 의하면 정부는 과세표준확정신고를 하여야 할 자에...   \n",
       "\n",
       "                                             summary  \n",
       "0  원고가  주동하여 회사업무능률을 저해하고 회사업무상의 지휘명령에 위반하였다면 이에 ...  \n",
       "1  수출입업체인 원고가 의류제품을 제조ㆍ수출함에 있어 소외 회사의 직수출실적을 지원하기...  \n",
       "2  가등기담보권자가 제소전 화해조항에 의해 자기 명의로 소유권이전의 본등기를 경료하고 ...  \n",
       "3  부가가치세법 제22조 제3항 단서에 제1호와 제2호가 동시에 해당한다는 의미는 제1...  \n",
       "4  소득세법 제116조 제1항에 따르면 정부는 과세표준확정신고를 해야 할 자에 대해 당...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7Nhgcjz4Tom",
    "outputId": "344c4d35-b6d7-498d-9760-ac7b2a40b24e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "vjD5LbTn4Tol",
    "outputId": "32dd95bb-8d48-4b2c-974d-3de64d1bd430"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[경주=환경일보] 강광태 기자 = 경주시는 창의적 아이디어를 적극 수렴함으로써 소통...</td>\n",
       "      <td>생성요약문</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>황영민 시장·군수·구청장 직접 제출… 경기도, 복지부·국회에 개선 요구 경기도가 장...</td>\n",
       "      <td>생성요약문</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>김대연 기자l saint-jj@hanmail.net 전북도의회는 올해 의회운영의 전...</td>\n",
       "      <td>생성요약문</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>대법원 2부(주심 안철상)는 스타 강사 우모씨를 상대로 인터넷 강의 업체 이투스교육...</td>\n",
       "      <td>생성요약문</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>사설 보호무역 확산, G20회의 철저한 준비를 1월 수출이 월별 수출입 동향을 집계...</td>\n",
       "      <td>생성요약문</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text summary\n",
       "0   1  [경주=환경일보] 강광태 기자 = 경주시는 창의적 아이디어를 적극 수렴함으로써 소통...   생성요약문\n",
       "1   2  황영민 시장·군수·구청장 직접 제출… 경기도, 복지부·국회에 개선 요구 경기도가 장...   생성요약문\n",
       "2   3  김대연 기자l saint-jj@hanmail.net 전북도의회는 올해 의회운영의 전...   생성요약문\n",
       "3   4  대법원 2부(주심 안철상)는 스타 강사 우모씨를 상대로 인터넷 강의 업체 이투스교육...   생성요약문\n",
       "4   5  사설 보호무역 확산, G20회의 철저한 준비를 1월 수출이 월별 수출입 동향을 집계...   생성요약문"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>생성요약문</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>생성요약문</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>생성요약문</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>생성요약문</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>생성요약문</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id summary\n",
       "0   1   생성요약문\n",
       "1   2   생성요약문\n",
       "2   3   생성요약문\n",
       "3   4   생성요약문\n",
       "4   5   생성요약문"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnoHGCZ74Ton"
   },
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NaWvcEQH4Too"
   },
   "outputs": [],
   "source": [
    "#결측치 제거\n",
    "nan_list = [27556, 34092, 34201, 66897, 133273, 216203]\n",
    "\n",
    "train_data = Train_data[~Train_data[\"id\"].isin(nan_list)].reset_index(drop = True)\n",
    "train_data.head()\n",
    "test_data = Test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['total'] = train_data[\"text\"] + ' ' + train_data[\"summary\"]  \n",
    "test_data['total'] = train_data[\"text\"] + ' ' + train_data[\"summary\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XM386Ifwh_gX"
   },
   "outputs": [],
   "source": [
    "encoder_len = 500\n",
    "decoder_len = 50\n",
    "max_vocab_size = 200000\n",
    "batch_size = 32\n",
    "num_layers = 6\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "epochs = 1\n",
    "learning_rate = 1e-4\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IE1hOD5-4Too"
   },
   "outputs": [],
   "source": [
    "train_data = train_data.iloc[:-4000, :]\n",
    "val_data = train_data.iloc[-4000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(267659, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "maOa_VS_ffLs"
   },
   "outputs": [],
   "source": [
    "class Mecab_Tokenizer():\n",
    "    def __init__(self, max_length, mode, max_vocab_size=-1):\n",
    "        self.text_tokenizer = Mecab()\n",
    "        self.mode = mode\n",
    "        self.txt2idx = {'pad_':0, 'unk_':1}\n",
    "        self.idx2txt = {0:'pad_', 1:'unk_'}\n",
    "        self.max_length = max_length\n",
    "        self.word_count = {}\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        \n",
    "        # 띄어쓰기를 찾기 위한 태그 목록\n",
    "        self.font_blank_tag = [\n",
    "            '', 'EC', 'EC+JKO', 'EF', 'EP+EC', 'EP+EP+EC', 'EP+ETM', 'EP+ETN+JKO', 'ETM', 'ETN', 'ETN+JKO', 'ETN+JX', 'IC', 'JC', 'JKB', 'JKB+JX', 'JKO',\n",
    "            'JKQ', 'JKS', 'JX', 'MAG', 'MAG+JX', 'MAG+XSV+EP+EC', 'MAJ','MM', 'MM+EC', 'NNB', 'NNB+JKB', 'NNB+JKO', 'NNB+VCP+EC', 'NNBC', 'NNG', 'NNG+JX+JKO',\n",
    "            'NNG+VCP+EC', 'NNP', 'NNP+JX', 'NP', 'NP+JKO', 'NP+JKS', 'NP+JX', 'NP+VCP+EC', 'NR', 'SC', 'SF', 'SL', 'SN', 'SSC', 'SSO', 'SY', 'UNKNOWN',\n",
    "            'VA+EC', 'VA+EC+VX+ETM', 'VA+ETM', 'VA+ETN+JKB+JX', 'VCN+EC', 'VCN+ETM', 'VCP', 'VCP+EC', 'VCP+EP+EC', 'VCP+EP+ETM', 'VCP+ETM', 'VCP+ETN',\n",
    "            'VV+EC', 'VV+EC+JX', 'VV+EC+VX+EC', 'VV+EC+VX+ETM', 'VV+EP+EC', 'VV+EP+ETM', 'VV+ETM', 'VV+ETN', 'VX+EC', 'VX+EC+VX+EP+EC', 'VX+EP+ETM',\n",
    "            'VX+ETM', 'XPN', 'XR', 'XSA+EC', 'XSA+EC+VX+ETM', 'XSA+ETM', 'XSN', 'XSV+EC', 'XSV+EP+EC', 'XSV+ETM', 'XSV+ETN', 'XSV+JKO'\n",
    "        ]\n",
    "        self.back_blank_tag = [\n",
    "            '', 'IC', 'MAG', 'MAG+JX', 'MAG+XSV+EP+EC', 'MAJ', 'MM', 'MM+EC', 'NNB', 'NNB+JKB', 'NNB+VCP', 'NNB+VCP+EC', 'NNB+VCP+EF', 'NNBC', 'NNBC+VCP+EC',\n",
    "            'NNG', 'NNG+JC', 'NNG+JX+JKO', 'NNG+VCP', 'NNG+VCP+EC', 'NNG+VCP+ETM', 'NNP', 'NNP+JX', 'NP', 'NP+JKG', 'NP+JKO', 'NP+JKS', 'NP+JX', 'NP+VCP+EC', 'NP+VCP+EF',\n",
    "            'NR', 'SC', 'SL', 'SN', 'SSC', 'SSO', 'SY', 'VA', 'VA+EC', 'VA+EC+VX+ETM', 'VA+EF', 'VA+ETM', 'VA+ETN', 'VA+ETN+JKB+JX', 'VCN', 'VCN+EC', 'VCN+EF', 'VCN+ETM',\n",
    "            'VCN+ETN', 'VCP', 'VCP+EF', 'VV', 'VV+EC', 'VV+EC+JX', 'VV+EC+VX', 'VV+EC+VX+EC', 'VV+EC+VX+EF', 'VV+EC+VX+EP+EC', 'VV+EC+VX+ETM', 'VV+EF', 'VV+EP', 'VV+EP+EC',\n",
    "            'VV+EP+ETM', 'VV+ETM', 'VV+ETN', 'VV+ETN+VCP+EF', 'VX', 'VX+ETM', 'XPN', 'XR', 'XSA+ETN+VCP+EF', 'XSN'\n",
    "        ]\n",
    "        \n",
    "    def morpheme(self, sentence_list):\n",
    "        new_sentence = []\n",
    "        for i, sentence in tqdm(enumerate(sentence_list)):\n",
    "            temp = []\n",
    "            if self.mode == 'dec':\n",
    "                temp.append('sos_')\n",
    "            for t in self.text_tokenizer.pos(sentence):\n",
    "                temp.append('_'.join(t))\n",
    "            if self.mode == 'dec':\n",
    "                temp.append('eos_')\n",
    "            new_sentence.append(' '.join(temp))\n",
    "            \n",
    "        return new_sentence\n",
    "    \n",
    "    def fit(self, sentence_list):\n",
    "        for sentence in tqdm(sentence_list):\n",
    "            for word in sentence.split(' '):\n",
    "                try:\n",
    "                    self.word_count[word] += 1\n",
    "                except:\n",
    "                    self.word_count[word] = 1\n",
    "        self.word_count = dict(sorted(self.word_count.items(), key=self.sort_target, reverse=True))\n",
    "        \n",
    "        self.txt2idx = {'pad_':0, 'unk_':1}\n",
    "        self.idx2txt = {0:'pad_', 1:'unk_'}\n",
    "        if self.max_vocab_size == -1:\n",
    "            for i, word in enumerate(list(self.word_count.keys())):\n",
    "                self.txt2idx[word]=i+2\n",
    "                self.idx2txt[i+2]=word\n",
    "        else:\n",
    "            for i, word in enumerate(list(self.word_count.keys())[:self.max_vocab_size]):\n",
    "                self.txt2idx[word]=i+2\n",
    "                self.idx2txt[i+2]=word\n",
    "        \n",
    "    def sort_target(self, x):\n",
    "        return x[1]\n",
    "            \n",
    "    def txt2token(self, sentence_list):\n",
    "        tokens = []\n",
    "        for sentence in tqdm(sentence_list):\n",
    "            token = [0]*self.max_length\n",
    "            for i, w in enumerate(sentence.split(' ')):\n",
    "                if i == self.max_length:\n",
    "                    break\n",
    "                try:\n",
    "                    token[i] = self.txt2idx[w]\n",
    "                except:\n",
    "                    token[i] = self.txt2idx['unk_']\n",
    "            tokens.append(token)\n",
    "        return np.array(tokens)\n",
    "    \n",
    "    def convert(self, token):\n",
    "        sentence = []\n",
    "        for j, i in enumerate(token):\n",
    "            if self.mode == 'enc':\n",
    "                if i != self.txt2idx['pad_']:\n",
    "                    sentence.append(self.idx2txt[i].split('_')[0])\n",
    "            elif self.mode == 'dec':\n",
    "                if i == self.txt2idx['eos_'] or i == self.txt2idx['pad_']:\n",
    "                    break\n",
    "                elif i != 0:\n",
    "                    sentence.append(self.idx2txt[i].split('_')[0])\n",
    "                    # 앞뒤 태그를 확인하여 띄어쓰기 추가\n",
    "                    if self.idx2txt[i].split('_')[1] in self.font_blank_tag:\n",
    "                        try:\n",
    "                            if self.idx2txt[token[j+1]].split('_')[1] in self.back_blank_tag:\n",
    "                                sentence.append(' ')\n",
    "                        except:\n",
    "                            pass\n",
    "        sentence = \"\".join(sentence)\n",
    "        if self.mode == 'enc':\n",
    "            sentence = sentence[:-1]\n",
    "        elif self.mode == 'dec':\n",
    "            sentence = sentence[3:-1]\n",
    "            \n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TQz-NXOvfk-j"
   },
   "outputs": [],
   "source": [
    "src_tokenizer = Mecab_Tokenizer(encoder_len, mode='enc', max_vocab_size=max_vocab_size)\n",
    "tar_tokenizer = Mecab_Tokenizer(decoder_len, mode='dec', max_vocab_size=max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wr_bKi2DfhJl",
    "outputId": "f014a929-d5b4-42ab-e41f-5d585479d980"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "267659it [03:26, 1295.54it/s]\n",
      "4000it [00:02, 1553.25it/s]\n",
      "10000it [00:12, 785.81it/s]\n",
      "267659it [01:27, 3069.96it/s]\n",
      "4000it [00:01, 3000.66it/s]\n"
     ]
    }
   ],
   "source": [
    "train_src = src_tokenizer.morpheme(train_data.total)\n",
    "val_src = src_tokenizer.morpheme(val_data.total)\n",
    "test_src = src_tokenizer.morpheme(test_data.total)\n",
    "\n",
    "train_tar = tar_tokenizer.morpheme(train_data.summary)\n",
    "val_tar = tar_tokenizer.morpheme(val_data.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 549
    },
    "id": "Kpa-4zg74Tov",
    "outputId": "d6b52c84-77f5-4d97-d0f9-23437ffdc54b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_src_max_len : 1062\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tar_max_len : 519\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_src_len = []\n",
    "for m in train_src:\n",
    "    m_len = len(m.split(' '))\n",
    "    train_src_len.append(m_len)\n",
    "print('train_src_max_len :', max(train_src_len))\n",
    "plt.hist(train_src_len, bins=30)\n",
    "plt.show()\n",
    "\n",
    "train_tar_len = []\n",
    "for m in train_tar:\n",
    "    m_len = len(m.split(' '))\n",
    "    train_tar_len.append(m_len)\n",
    "print('train_tar_max_len :', max(train_tar_len))\n",
    "plt.hist(train_tar_len, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rfLacPsgOJ2",
    "outputId": "0177a6bf-4479-40e2-ccd1-52b454e44ec4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267659/267659 [00:12<00:00, 21473.23it/s]\n",
      "100%|██████████| 267659/267659 [00:04<00:00, 59256.64it/s]\n"
     ]
    }
   ],
   "source": [
    "src_tokenizer.fit(train_src)\n",
    "tar_tokenizer.fit(train_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQ1sHA0DgQWh",
    "outputId": "732a5120-79c4-44ed-85d0-cd50b43431d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267659/267659 [00:20<00:00, 13316.68it/s]\n",
      "100%|██████████| 4000/4000 [00:00<00:00, 17512.90it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 11606.71it/s]\n",
      "100%|██████████| 267659/267659 [00:06<00:00, 44413.03it/s]\n",
      "100%|██████████| 4000/4000 [00:00<00:00, 52336.39it/s]\n"
     ]
    }
   ],
   "source": [
    "train_src_tokens = src_tokenizer.txt2token(train_src)\n",
    "val_src_tokens = src_tokenizer.txt2token(val_src)\n",
    "test_src_tokens = src_tokenizer.txt2token(test_src)\n",
    "\n",
    "train_tar_tokens = tar_tokenizer.txt2token(train_tar)\n",
    "val_tar_tokens = tar_tokenizer.txt2token(val_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "rCOnJT1-gSOm"
   },
   "outputs": [],
   "source": [
    "input_vocab_size = len(src_tokenizer.txt2idx)\n",
    "target_vocab_size = len(tar_tokenizer.txt2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3cwb4oBkgUbl",
    "outputId": "3dec7a40-c041-4cde-9f15-e1e0d3b7c73c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178516, 124526)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab_size, target_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "MYaunVUbgV96",
    "outputId": "93f6567e-550a-4536-9b8d-c5ee7162c938"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'원고가  주동하여 회사업무능률을 저해하고 회사업무상의 지휘명령에 위반하였다면 이에 따른 징계해고는 사내질서를 유지하기 위한 사용자 고유의 정당한 징계권의 행사로 보아야 한다.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.summary.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ACMG1R-TgaY-",
    "outputId": "3f54cef7-8340-4148-dbce-36ef8869e024"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    8,  1148,    16, 31948,     6,    42,   276,   260, 16532,\n",
       "            2,  4516,     6,    13,   276,   260,   178,     3, 40477,\n",
       "            4,   476,     6,   112,   417,    69,     4,   473,  1401,\n",
       "         1798,    17,  4118,  2452,     5,   645,     6,    35,    92,\n",
       "          190,   161,  2689,     3,  1154,    23, 15622,     3,   202,\n",
       "           22,   302,   437,    82,    10]),\n",
       " ' 원고가 주동하여 회사 업무 능률을 저해하고 회사 업무 상의지휘명령에 위반하였다면 이에 따른 징계 해고는 사내 질서를 유지하기 위한 사용 자 고유의정당한 징계권의행사로 보아야한다')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tar_tokens[0], tar_tokenizer.convert(train_tar_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ZxdKx8nGgfo8"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, src_tokens, tar_tokens, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.src_tokens = src_tokens\n",
    "        if self.mode == 'train':\n",
    "            self.tar_tokens = tar_tokens\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.src_tokens)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        src_token = self.src_tokens[i]\n",
    "        if self.mode == 'train':\n",
    "            tar_token = self.tar_tokens[i]\n",
    "            return {\n",
    "                'src_token' : torch.tensor(src_token, dtype=torch.long),\n",
    "                'tar_token' : torch.tensor(tar_token, dtype=torch.long),\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'src_token' : torch.tensor(src_token, dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Chr56qWFgg7s"
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_src_tokens, train_tar_tokens)\n",
    "val_dataset = CustomDataset(val_src_tokens, val_tar_tokens)\n",
    "test_dataset = CustomDataset(test_src_tokens, None, 'test')\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=1, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=1, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "i5VAxb8OgjUn"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "HRDxIlx2glYR"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return torch.tensor(pos_encoding, dtype=torch.float32)\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = torch.tensor(torch.eq(seq, 0), dtype=torch.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    seq = seq.unsqueeze(1).unsqueeze(2)\n",
    "    return seq  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = torch.ones(size, size).triu(diagonal=1)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = torch.matmul(q, torch.transpose(k, -2, -1))  # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    # scale matmul_qk\n",
    "    dk = k.size()[-1]\n",
    "    scaled_attention_logits = matmul_qk / math.sqrt(dk)\n",
    "    \n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = torch.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print('Attention weights are:')\n",
    "    print(temp_attn)\n",
    "    print('Output is:')\n",
    "    print(temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIpdd7b9gnWT",
    "outputId": "602505c3-b03c-4833-bb34-8d808e9e3cd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tensor([[8.4333e-26, 1.0000e+00, 8.4333e-26, 8.4333e-26]])\n",
      "Output is:\n",
      "tensor([[1.0000e+01, 9.2766e-25]])\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = torch.tensor([[10, 0, 0],\n",
    "                      [0, 10, 0],\n",
    "                      [0, 0, 10],\n",
    "                      [0, 0, 10]], dtype=torch.float32)  # (4, 3)\n",
    "\n",
    "temp_v = torch.tensor([[1, 0],\n",
    "                      [10, 0],\n",
    "                      [100, 5],\n",
    "                      [1000, 6]], dtype=torch.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = torch.tensor([[0, 10, 0]], dtype=torch.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pACCw3tgo4v",
    "outputId": "09fe3f5b-c9a6-45a4-e0eb-7b2d73ff956b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tensor([[5.0000e-01, 5.0000e-01, 4.2166e-26, 4.2166e-26]])\n",
      "Output is:\n",
      "tensor([[5.5000e+00, 4.6383e-25]])\n"
     ]
    }
   ],
   "source": [
    "temp_q = torch.tensor([[10, 10, 0]], dtype=torch.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BsKkiW68grVb",
    "outputId": "765a24dd-9151-4ca8-8798-a739f706724b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tensor([[4.2166e-26, 4.2166e-26, 5.0000e-01, 5.0000e-01],\n",
      "        [8.4333e-26, 1.0000e+00, 8.4333e-26, 8.4333e-26],\n",
      "        [5.0000e-01, 5.0000e-01, 4.2166e-26, 4.2166e-26]])\n",
      "Output is:\n",
      "tensor([[5.5000e+02, 5.5000e+00],\n",
      "        [1.0000e+01, 9.2766e-25],\n",
      "        [5.5000e+00, 4.6383e-25]])\n"
     ]
    }
   ],
   "source": [
    "temp_q = torch.tensor([[0, 0, 10],\n",
    "                      [0, 10, 0],\n",
    "                      [10, 10, 0]], dtype=torch.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-1TCnU9ZgtRZ"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, v, k, q, mask):\n",
    "        batch_size = q.size()[0]\n",
    "        \n",
    "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = scaled_attention.transpose(1,2).contiguous().view(batch_size, -1, self.num_heads * self.depth)\n",
    "                \n",
    "        output = self.wo(scaled_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-L8m1mGNguVa",
    "outputId": "1ad06cb1-c7dc-46db-901f-afda5d74adb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 60, 512]), torch.Size([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = torch.rand(1, 60, 512)  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "rI5nDFD8gzHa"
   },
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model, dff):\n",
    "        super(FFN, self).__init__()\n",
    "        self.layer1 = nn.Linear(d_model, dff)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc = nn.Linear(dff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "6uKsRBIxg0c9"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, maximum_position_encoding, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FFN(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm([maximum_position_encoding, d_model])\n",
    "        self.layernorm2 = nn.LayerNorm([maximum_position_encoding, d_model])\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "H0nXqKxmg1rx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 500, 512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048, encoder_len)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    torch.rand(64, encoder_len, 512), None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "0fBWJbEzg3If"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, maximum_position_encoding, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = FFN(d_model, dff)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "        self.dropout3 = nn.Dropout(rate)\n",
    "        \n",
    "        self.layernorms1 = nn.ModuleList([copy.deepcopy(nn.LayerNorm([i+1, d_model])) for i in range(maximum_position_encoding)])\n",
    "        self.layernorms2 = nn.ModuleList([copy.deepcopy(nn.LayerNorm([i+1, d_model])) for i in range(maximum_position_encoding)])\n",
    "        self.layernorms3 = nn.ModuleList([copy.deepcopy(nn.LayerNorm([i+1, d_model])) for i in range(maximum_position_encoding)])\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        out1 = self.layernorms1[x.size(1)-1](attn1 + x)\n",
    "        \n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorms2[x.size(1)-1](attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorms3[x.size(1)-1](ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "zEvF0s6Bg5bK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 50, 512])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048, decoder_len)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    torch.rand(64, decoder_len, 512), sample_encoder_layer_output,\n",
    "    None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "s283AQ5JhBy8"
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "GQtyQLhShF2g"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, device, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model).to(device)\n",
    "        \n",
    "        self.dec_layers = clones(EncoderLayer(d_model, num_heads, dff, maximum_position_encoding, rate), num_layers)\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, x, mask, enc_output=None):\n",
    "        if enc_output == None:\n",
    "            seq_len = x.size()[1]\n",
    "            attention_weights = {}\n",
    "            x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "            x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "            x += self.pos_encoding[:, :seq_len, :]\n",
    "            x = self.dropout(x)\n",
    "            for i in range(self.num_layers):\n",
    "                x = self.dec_layers[i](x, mask)\n",
    "        else:\n",
    "            x = enc_output\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "svvbquUGhGhT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 500, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                         dff=2048, input_vocab_size=input_vocab_size,\n",
    "                         maximum_position_encoding=encoder_len,\n",
    "                         device='cpu')\n",
    "\n",
    "temp_input = torch.randint(low=0, high=input_vocab_size, size=(64, encoder_len))\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, mask=None, enc_output=None)\n",
    "\n",
    "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "s_XaHooshJ3P"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, device, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model).to(device)\n",
    "        \n",
    "        self.dec_layers = clones(DecoderLayer(d_model, num_heads, dff, maximum_position_encoding, rate), num_layers)\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "        \n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        seq_len = x.size()[1]\n",
    "        attention_weights = {}\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "            \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "pBHXPZ8lhLIw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 50, 512]), torch.Size([64, 8, 50, 500]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                         dff=2048, target_vocab_size=target_vocab_size,\n",
    "                         maximum_position_encoding=decoder_len,\n",
    "                         device='cpu')\n",
    "\n",
    "temp_input = torch.randint(low=0, high=target_vocab_size, size=(64, decoder_len))\n",
    "\n",
    "output, attn = sample_decoder(temp_input,\n",
    "                              enc_output=sample_encoder_output,\n",
    "                              look_ahead_mask=None,\n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "PxggQEMahQ4w"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               target_vocab_size, pe_input, pe_target, device, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                                 input_vocab_size, pe_input, device, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                               target_vocab_size, pe_target, device, rate)\n",
    "\n",
    "        self.final_layer = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inp, tar, enc_output = inputs\n",
    "\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
    "\n",
    "        enc_output = self.encoder(inp, enc_padding_mask, enc_output)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights, enc_output\n",
    "\n",
    "    def create_masks(self, inp, tar):\n",
    "        # Encoder padding mask\n",
    "        enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 2nd attention block in the decoder.\n",
    "        # This padding mask is used to mask the encoder outputs.\n",
    "        dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = create_look_ahead_mask(tar.size(1))\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\n",
    "        look_ahead_mask = torch.maximum(dec_target_padding_mask.to(self.device), look_ahead_mask.to(self.device))\n",
    "\n",
    "        return enc_padding_mask, look_ahead_mask, dec_padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "qmiokLXuhSjH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 50, 124526])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048,\n",
    "    input_vocab_size=input_vocab_size, target_vocab_size=target_vocab_size,\n",
    "    pe_input=encoder_len, pe_target=decoder_len, device='cpu')\n",
    "\n",
    "temp_input = torch.randint(low=0, high=input_vocab_size, size=(64, encoder_len))\n",
    "temp_target = torch.randint(low=0, high=target_vocab_size, size=(64, decoder_len))\n",
    "\n",
    "fn_out, _, _ = sample_transformer([temp_input, temp_target, None])\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "JU1R2SHshVNO"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=input_vocab_size,\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    pe_input=encoder_len,\n",
    "    pe_target=decoder_len-1,\n",
    "    device=device,\n",
    "    rate=dropout_rate\n",
    ")\n",
    "\n",
    "transformer = transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "1gBSoIFKhXM2"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "qadaLyUphYkq"
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = torch.logical_not(torch.eq(real, 0))\n",
    "    loss_ = criterion(pred.permute(0,2,1), real)\n",
    "    mask = torch.tensor(mask, dtype=loss_.dtype)\n",
    "    loss_ = mask * loss_\n",
    "\n",
    "    return torch.sum(loss_)/torch.sum(mask)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = torch.eq(real, torch.argmax(pred, dim=2))\n",
    "    mask = torch.logical_not(torch.eq(real, 0))\n",
    "    accuracies = torch.logical_and(mask, accuracies)\n",
    "    accuracies = torch.tensor(accuracies, dtype=torch.float32)\n",
    "    mask = torch.tensor(mask, dtype=torch.float32)\n",
    "    \n",
    "    return torch.sum(accuracies)/torch.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "TjGFveZNhZ5T"
   },
   "outputs": [],
   "source": [
    "def train_step(batch_item, epoch, batch, training):\n",
    "    src = batch_item['src_token'].to(device)\n",
    "    tar = batch_item['tar_token'].to(device)\n",
    "    \n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    \n",
    "    if training is True:\n",
    "        transformer.train()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output, _, _ = transformer([src, tar_inp, None])\n",
    "            loss = loss_function(tar_real, output)\n",
    "        acc = accuracy_function(tar_real, output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        return loss, acc, round(lr, 10)\n",
    "    else:\n",
    "        transformer.eval()\n",
    "        with torch.no_grad():\n",
    "            output, _, _ = transformer([src, tar_inp, None])\n",
    "            loss = loss_function(tar_real, output)\n",
    "        acc = accuracy_function(tar_real, output)\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUmKQle1hban",
    "outputId": "1d9ac633-2466-4305-feb3-898167703286"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8365it [47:43,  2.92it/s, Epoch=1, LR=0.0001, Loss=5.518973, Total Loss=5.213784, Total ACC=0.282596]\n",
      "125it [00:15,  7.91it/s, Epoch=1, Val Loss=5.306769, Total Val Loss=4.861233, Total Val ACC=0.372737]\n"
     ]
    }
   ],
   "source": [
    "loss_plot, val_loss_plot = [], []\n",
    "acc_plot, val_acc_plot = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    gc.collect()\n",
    "    total_loss, total_val_loss = 0, 0\n",
    "    total_acc, total_val_acc = 0, 0\n",
    "    \n",
    "    tqdm_dataset = tqdm(enumerate(train_dataloader))\n",
    "    training = True\n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        batch_loss, batch_acc, lr = train_step(batch_item, epoch, batch, training)\n",
    "        total_loss += batch_loss\n",
    "        total_acc += batch_acc\n",
    "        \n",
    "        tqdm_dataset.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'LR' : lr,\n",
    "            'Loss': '{:06f}'.format(batch_loss.item()),\n",
    "            'Total Loss' : '{:06f}'.format(total_loss/(batch+1)),\n",
    "            'Total ACC' : '{:06f}'.format(total_acc/(batch+1))\n",
    "        })\n",
    "    loss_plot.append(total_loss/(batch+1))\n",
    "    acc_plot.append(total_acc/(batch+1))\n",
    "    \n",
    "    tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
    "    training = False\n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        batch_loss, batch_acc = train_step(batch_item, epoch, batch, training)\n",
    "        total_val_loss += batch_loss\n",
    "        total_val_acc += batch_acc\n",
    "        \n",
    "        tqdm_dataset.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Val Loss': '{:06f}'.format(batch_loss.item()),\n",
    "            'Total Val Loss' : '{:06f}'.format(total_val_loss/(batch+1)),\n",
    "            'Total Val ACC' : '{:06f}'.format(total_val_acc/(batch+1))\n",
    "        })\n",
    "    val_loss_plot.append(total_val_loss/(batch+1))\n",
    "    val_acc_plot.append(total_val_acc/(batch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "A8yHooxmhc2C"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot, label='train_loss')\n",
    "plt.plot(val_loss_plot, label='val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "h5S9gRKWhd10"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_plot, label='train_acc')\n",
    "plt.plot(val_acc_plot, label='val_acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "3T4awuawhfJB"
   },
   "outputs": [],
   "source": [
    "def evaluate(tokens):\n",
    "    transformer.to(device)\n",
    "    decoder_input = torch.tensor([tar_tokenizer.txt2idx['sos_']] * tokens.size(0), dtype=torch.long).to(device)\n",
    "    output = decoder_input.unsqueeze(1).to(device)\n",
    "    enc_output = None\n",
    "    for i in range(decoder_len-1):        \n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        with torch.no_grad():\n",
    "            predictions, attention_weights, enc_output = transformer([tokens, output, enc_output])\n",
    "        \n",
    "        # select the last token from the seq_len dimension\n",
    "        predictions_ = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "        \n",
    "        predicted_id = torch.tensor(torch.argmax(predictions_, axis=-1), dtype=torch.int32)\n",
    "        \n",
    "        output = torch.cat([output, predicted_id], dim=-1)\n",
    "    output = output.cpu().numpy()\n",
    "    \n",
    "    summary_list = []\n",
    "    token_list = []\n",
    "    for token in output:\n",
    "        summary = tar_tokenizer.convert(token)\n",
    "        summary_list.append(summary)\n",
    "        token_list.append(token)\n",
    "    return summary_list, token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "dSOSSyKWhggo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [03:11,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
    "preds = []\n",
    "tokens = []\n",
    "for batch, batch_item in tqdm_dataset:\n",
    "    output = evaluate(batch_item['src_token'].to(device))\n",
    "    preds.extend(output[0])\n",
    "    tokens.extend(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "eB6Os4Dnhh0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답 : 미국여자프로골프(LPGA) 투어 KPMG 위민스 PGA 챔피언십이 하루 앞으로 다가오면서 고진영(24) 이정은(23)이 앞선 두메이저 대회 우승,지난3월 HSBC 위민스 월드챔피언십에서 통산 6번 우승한  박성현(26)의  대회 2연패 달성 여부 박세리와 함께 최다승(3승)인 박인비(31)의 네번째 우승 여부등 한국 시스터스 강세에 메이저 3연승 달성에 관심이 주목된다.\n",
      "예측 :  미국이 미국 프로 선수 권 대회에서 열리는 선수 들이 미국 여자 선수 권 대회에 참가한 결과 , 미국 여자 선수 권 대회에 참가한 선수 권 대회에서 3 월 ( 3 월 )이 3 월 ) ( 3 월 \n",
      "=================================================================================\n",
      "정답 : 1991년 세계 최초로 기내 흡연을 금지하며 국내 최초 금연 기업이 된 아시아나항공이 24년 만에 기내면세점 담배 판매를 재개한 것은 수익성에 대한 절박함이 작용한 것으로 보여진다.\n",
      "예측 :  기업은 기업 투자를 통해 영업 이익이 높은 영업 이익 성 조사 결과로 인해 영업 이익이 높은 영업 이익 률이 전년 동기 대비 0 . 9 % 증가한 것은 전년 동기 대비 0 . 9 % 감소한 것으\n",
      "=================================================================================\n",
      "정답 : 독일 명문 악단 쾰른 귀르체니히 오케스트라의 종신수석 풀루티스트 조성현과 세계적인 명장 다니엘 바렌보임이 이끄는 독일 베를린 슈타츠카펠레의 첫 여성 종신악장 바이올리니스트 이지윤 등 세계적으로 유명한 젊은 한국 음악가들이 고국의 여름밤을 수놓는데 이들은 금호아시아나문화재단의 음악영재 발굴, 육성 프로그램인 '금호영재콘서트'출신이다.\n",
      "예측 :  우리 나라 들은 세계 적인 문화 예술 문화 예술의삶이 될 수 있는 세계 적인 문화 예술 문화 예술 문화 예술 문화 예술 문화 예술의삶의삶이 될 수 있는 문화 예술 문화 예술 문화 예술 문화 예술\n",
      "=================================================================================\n",
      "정답 : 18일 영화진흥위원회에 의하면 '알라딘' 이 하루 관객수 13만 명을 넘어서며 역주행해 17일 만에 '기생충' 을 누르고 1위에 올라서 흥행에 성공하였다.\n",
      "예측 :  1 명이 1 일을 기록해 1 만 명 명에 1 만 명 이상 1 명을 기록해 1 만 명에 1 만 명 중 1 명 중 1 명 중 1 명 중 1 명 중 1 명 중 1 명 중 \n",
      "=================================================================================\n",
      "정답 : 인사처 시험출제과 직원들은 합숙 생활이 일상이라 힘들지만 인재 뽑는 사명감으로 일하는데 공시 출제 관련 에피소드 중 월세만 내고 한 달 내내 집에 들어가 보지 못할 때가 많아 집주인이 경찰에 신고한 경우도 있었다.\n",
      "예측 :  한 사람 들은 학교도 공무원 들에게도 불구하고 학교 공무원 들에게도 공무원 들에게 피해한 피해가 있는 학교 지원 대책과 관련한 지원 대책은 물론 공무원 들에게도 불구하지만 공무원 들에게도 불\n",
      "=================================================================================\n",
      "정답 : 18일 정순균 강남구청장은 정부가 미세먼지 저감에 대해 강력한 의지를 가져야 한다며 경유 차량 보조금 지급과 휘발유 보다 저렴한 경유 가격이 유지되면 경유 차량을 포기하는 게 쉽지 않기 때문에 정부의 경유 차량 혜택 폐지를 주장했다.\n",
      "예측 :  정부가 대책과 정부가 미세먼지 저감 대책에 대한 대책과 정부가 발생한 정부에 대한 대책 마련과 정부가 필요한 대책 마련에 대한 대책 마련이 필요한 대책 마련에 따라 정부가 필요한 대책 마련과 정부\n",
      "=================================================================================\n",
      "정답 : 17일, 한국의약품안전관리원은 의약품 부작용 피해구제 신청처리 현황을 공개하여 의약품 부작용 피해구제 제도가 실시된 뒤로 피해구제 신청건수가 계속 증가하고 있는 것으로 나타났다며 2015년 20건에서 2018년 139건, 2019년 4월 말 기준 50건으로 4년 동안 평균 90.8% 상승하였다.\n",
      "예측 :  고용 노동부가 고용 노동부에서 발생하여 고용 노동부 조사 결과가 발생하여 고용 노동부 발생 률은 전년 대비 0 % 증가한 기준 금리 증가로 인한 임금 인상 률이 전년 대비 0 . 9 % 증가한 결과로 인한 피\n",
      "=================================================================================\n",
      "정답 : 울창한 숲과 수려한 휴양림보다는 바다에 연접한 변산, 단독 펜션 형태의 숲속집 대야산, 독립적 야영데크의 인기가 높아지며 국립자연휴양림이 양적 확대와 함께 다양한 수요 반영으로 질적 변화를 진행해 휴양림 이용 경재에 뚜렷한 변화가 감지되고 있으나 일부에서는 휴양림의 취지가 상실되는 게 아니냐는 우려의 목소리도 들린다.\n",
      "예측 :  우리 나라가 건강과 건강한 건강 관리를 위해 다양한 건강 보험과 건강한 건강한 건강한 건강한 건강과 건강한 건강한 건강한 건강한 건강 관리로 인해 건강한 건강한 건강 보험과 건강의건강\n",
      "=================================================================================\n",
      "정답 : 서울 성북구 학생 1500여명은 성북구 우호도시인  미국 글렌데일시 시민들에게 일본군 위안부 관련 평화의 소녀상을 건립한 것에 감사를 표하는 편지를 썼다.\n",
      "예측 :  한 일 관계가 어제 한 일 관계 정상 회담을 통해 북한에게 한 일 대 총선에서 한 일 대 총선을 갖는 등 북한에게 한 일 대 총선의한 일 대 총선과 관련해 온 국가 들에게 한 일 대\n",
      "=================================================================================\n",
      "정답 : 지난달 24일 구로구 개봉1동 주민센터에서 열린 '우리동네 협치 산책, 찾아가는 릴레이 공론장'에서 주민들이 토론을 통해 지역 현안 의제를 선정하고 있으며, 민관협치 원년을 맞아 주민들과 비전을 공유하고 지역의 주요안건에 대해 나누는 시간을 통해 구로구 협치회의, 각 분야 활동가, 주민들이 이번 행사를 통해 비전선포식과 대공론을 진행할 계획이다.\n",
      "예측 :  주민 들은 주민 참여 자가 주민 참여 자 들의의견 수렴과 주민 참여를 위해 주민 참여 자 들이 주민 참여 자를 방문하는 주민 참여 자 들에게 주민 참여 방안 수립 방안 수립 방안을 촉구하고 , \n",
      "=================================================================================\n",
      "정답 : 서울 서초구가 전국 최초로 초·중·고교 53곳의 통학로 등 학교 경계 10m 이내를 금연구역으로 지정하고, 또 흡연자와 비흡연자 간 사회적 갈등을 줄이는 방안으로 '개방형 흡연부스' 와 '라인형 흡연구역'도 시범 운영하고, 관련 구청장은 담배 연기 없는 청정 서초를 만들어 가겠다고 말했다.\n",
      "예측 :  교육부가 학교 운영 대책은 학교를 대상으로 학교 학교 내에 대한 피해를 줄이기 위해 학교 학교 내 학교 들의안전 대책 ' 대책 '은 물론 학교 폭력 대책 마련 등으로 인해 학교 폭력 대책 마련은 물론 학교 발생\n",
      "=================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i, (a, p) in enumerate(zip(val_data.summary, preds)):\n",
    "    print('정답 :', a)\n",
    "    print('예측 :', p)\n",
    "    print('=================================================================================')\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [07:58,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "tqdm_dataset = tqdm(enumerate(test_dataloader))\n",
    "preds = []\n",
    "tokens = []\n",
    "for batch, batch_item in tqdm_dataset:\n",
    "    output = evaluate(batch_item['src_token'].to(device))\n",
    "    preds.extend(output[0])\n",
    "    tokens.extend(output[1])\n",
    "    \n",
    "submission = pd.read_csv('open/sample_submission.csv')\n",
    "submission['summary'] = preds\n",
    "submission.head()\n",
    "submission.to_csv('dacon_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dacon baseline.ipynb의 사본",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
